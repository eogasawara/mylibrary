{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in library(\"datamodelr\"): there is no package called 'datamodelr'\n",
     "output_type": "error",
     "traceback": [
      "Error in library(\"datamodelr\"): there is no package called 'datamodelr'\nTraceback:\n",
      "1. library(\"datamodelr\")"
     ]
    }
   ],
   "source": [
    "source(\"https://raw.githubusercontent.com/eogasawara/mylibrary/master/myGraphics.R\")\n",
    "#devtools::install_github(\"bergant/datamodelr\")\n",
    "library(\"datamodelr\")\n",
    "loadlibrary(\"DiagrammeR\")\n",
    "loadlibrary(\"dplyr\")\n",
    "loadlibrary(\"ggraph\")\n",
    "loadlibrary(\"igraph\")\n",
    "loadlibrary(\"ggplot2\")\n",
    "plot_size(4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing of data is almost always necessary as a prerequisite prior to the processing of some method or methods used in data science, such as machine learning, network analysis, among others.\n",
    "\n",
    "The need is generated by the fact that in most cases, for the purpose of the analysis, pieces of information contained in several databases are needed. These databases may contain incomplete information or are in undesirable format for the processing itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a simple example of preprocessing data using flight information, airports, airlines and weather conditions over a period of time.\n",
    "\n",
    "Let's say that the purpose of preprocessing is to obtain a concise dataset of flight data containing origin, date, time of delay and indication of weather conditions on the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we load information from different databases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reads files from airports, airlines, flights and weather conditions.\n",
    "airlines <- read.csv(file=\"airline.csv\", header=TRUE, sep=\",\")\n",
    "airports <- read.csv(file=\"airport.csv\", header=TRUE, sep=\",\")\n",
    "flights <- read.csv(file=\"flights.csv\", header=TRUE, sep=\",\")\n",
    "weather <- read.csv(file=\"weather.csv\", header=TRUE, sep=\",\")\n",
    "\n",
    "\n",
    "head(airlines)\n",
    "head(airports)\n",
    "head(flights)\n",
    "head(weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mount data model\n",
    "dm_f <- dm_from_data_frames(flights, airlines, weather, airports)\n",
    "\n",
    "\n",
    "dm_f <- dm_add_references(\n",
    "  dm_f,\n",
    "  \n",
    "  flights$AIRLINE == airlines$IATA,\n",
    "  flights$ORIGIN_AIRPORT == airports$IATA_CODE,\n",
    "  flights$DESTINATION_AIRPORT == airports$IATA_CODE,\n",
    "  weather$IATA == airports$IATA_CODE\n",
    ")\n",
    "graph <- dm_create_graph(dm_f, rankdir = \"BT\", col_attr = c(\"column\", \"type\"))\n",
    "dm_render_graph(graph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction - In our reduction example, we selected four columns out of the twenty three in the flight database. We selected only the columns that would be interesting for the purpose of the example study, which follow: year, day, month, airline, airport of origin and departure delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#reduction - only use fields for the desired job\n",
    "red <- data.frame(YEAR = flights$YEAR, MONTH = flights$MONTH, \n",
    "                  DAY = flights$DAY, AIRLINE = flights$AIRLINE, \n",
    "                  ORIGIN_AIRPORT = flights$ORIGIN_AIRPORT, \n",
    "                  DEPARTURE_DELAY = flights$DEPARTURE_DELAY)\n",
    "\n",
    "head(red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning - In our example of cleaning, we disregard all flight records where one of the basic and mandatory information we identified was null. If the information is null, it will not be useful in our example study, so it was discarded. Other types of cleanup could be applied, such as identifying information that does not conform to the expected format or type. For example, a field in which we expect to find a date, however comes full in the base as a list of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#cleaning - remove any record with at least one of the fields equal to null\n",
    "clr <- na.omit(red)\n",
    "head(clr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation - In our example of transformation, we use some of the fields from the base of climatic conditions and we make a synthetic evaluation of the situation in the day in question. In this case, we analyzed rainfall information on the day, amount of rainfall and fog level to generate a unique climatic condition assessment between the values \"ok\" and \"alert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transformation - to create new dataframe with flight and weather data\n",
    "trweather <- weather\n",
    "trweather$condition <- NA\n",
    "for(i in 1:nrow(trweather)){\n",
    "  trweather$condition[i] <- if ((trweather$RAINTODAY[i] == \"Yes\") && (trweather$RAINFALL[i] > 0.2) && (trweather$CLOUD9AM[i] > 7)) {\"Alert\"} else {\"Ok\"}\n",
    "}\n",
    "head(trweather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Integration - In our integration example, we integrate the already reduced and clean bases of flights and weather conditions on a single basis that will be the database for processing. We linked the bases to the airport information of origin and date, generating a unique flight view and weather condition on the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#integration - Integrates the bases of flights and climate through the airport code, and date on which the flight occurred. \n",
    "mer <- merge(clr, trweather, by.x=c(\"ORIGIN_AIRPORT\",\"YEAR\",\"MONTH\",\"DAY\"), by.y=c(\"IATA\",\"YEAR\",\"MONTH\",\"DAY\"))\n",
    "basedata <- data.frame(ORIGIN_AIRPORT = mer$ORIGIN_AIRPORT, MONTH =mer$MONTH, \n",
    "                       DAY = mer$DAY, YEAR = mer$YEAR, AIRLINE = mer$AIRLINE, \n",
    "                       DEPARTURE_DELAY =mer$DEPARTURE_DELAY, WEATHER_CONDITION = mer$condition)\n",
    "\n",
    "dresult <- dm_from_data_frames(basedata)\n",
    "graph <- dm_create_graph(dresult, rankdir = \"BT\", col_attr = c(\"column\", \"type\"))\n",
    "dm_render_graph(graph)\n",
    "\n",
    "basedata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"https://raw.githubusercontent.com/eogasawara/mylibrary/master/myGraphics.R\")\n",
    "loadlibrary(\"caret\")\n",
    "loadlibrary(\"cluster\")\n",
    "plot_size(4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering is the set of techniques that allows us to analyze and classify data by similarity / similarity. Clustering is a common technique for market segmentation because it finds similar groups according to the dataset. We can, for example, use grouping for market segmentation dive clients into smaller and more similar groups, and then design the marketing strategy specifically for each group.\n",
    "\n",
    "For example, an airline is trying to learn more about its customers so that it can segment different customer segments with different types of mileage offerings.\n",
    "\n",
    "In our example, we set up a synthetic database with three information, namely:\n",
    "\n",
    "miles = number of miles miles12 = miles in the last 12 months flights = number of flights in the last 12 months daysprogram = days since enrolled the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read source data\n",
    "mileage <- read.csv(\"cluster.csv\")\n",
    "summary(mileage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(mileage, 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing data\n",
    "preproc <-  preProcess(mileage)\n",
    "mileageNorm <- predict(preproc, mileage)\n",
    "summary(mileageNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distances between data points (using euclidean distance) \n",
    "distances <- dist(mileageNorm, method = \"euclidean\")\n",
    "\n",
    "# Hierarchical clustering\n",
    "mileageClust <- hclust(distances, method = \"ward.D\") \n",
    "\n",
    "plot(mileageClust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-Means Clustering with 5 clusters\n",
    "fit <- kmeans(mileageNorm, 5)\n",
    "\n",
    "# Cluster Plot against 1st 2 principal components\n",
    "\n",
    "# vary parameters for most readable graph\n",
    "clusplot(mileageNorm, fit$cluster, color=TRUE, shade=TRUE, \n",
    "         labels=2, lines=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example: Cluster 1 with the largest average values in DaysSinceEnroll. Cluster 2 with the largest average values in miles. Cluster 3 with customers have lower than average values in all variables. Cluster 4 with customers with few miles, but who have been with the airline the longest. Cluster 5 with the greater average values in miles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Learning - Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"https://raw.githubusercontent.com/eogasawara/mylibrary/master/myGraphics.R\")\n",
    "library(\"ggplot2\")\n",
    "library(\"reshape\")\n",
    "library(\"base\")\n",
    "library(\"stats\")\n",
    "library(\"dplyr\")\n",
    "library(\"neuralnet\")\n",
    "\n",
    "plot_size(4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks is inspired by the human brain to performs a particular task or functions. Neural Network (or Artificial Neural Network) has the ability to learn by examples. The process of learning occurs through a set of connected input/output units in which each connection has a weight associated with it. In the learning phase, the network learns by adjusting the weights to predict the correct class label of the given inputs. The hidden layer where processing takes place through the application of mathematical functions and weight adjustments needs to be trained so that the results are adequate to the input data.\n",
    "\n",
    "To iniciate we create a dataset with random information exemplifying flight data. For each of these six flights, the table contains the TTD Time to Departure (original time in minutes marked for departure) and the TSA Time since Arrival (time in minutes since the aircraft arrived on the ground). They are information that, when looking at the input data, we can infer the result, which is not used to train the processing layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating trainning data set\n",
    "ttd=c(20,10,15,20,80,30)\n",
    "tsa=c(5,30,15,50,50,80)\n",
    "delayed=c(1,1,1,0,0,0)\n",
    "\n",
    "# single dataset combining multiple columns\n",
    "df=data.frame(ttd,tsa,delayed)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create our Neural Network using the trainning data above (df)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network\n",
    "# hidden=3: represents single layer with 3 neurons respectively.\n",
    "nn=neuralnet(delayed~ttd+tsa,data=df, hidden=3,act.fct = \"logistic\",\n",
    "             linear.output = FALSE)\n",
    "plot(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we proccess with the teste data in the trained NN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "ttd=c(30,10,20)\n",
    "tsa=c(85,5,10)\n",
    "test=data.frame(ttd,tsa)\n",
    "\n",
    "\n",
    "## Prediction delay for the flights considering the NN trained\n",
    "Predict=compute(nn,test)\n",
    "Predict$net.result\n",
    "\n",
    "# Converting into final results indicating delay (1)yes / (0)no\n",
    "prob <- Predict$net.result\n",
    "pred <- ifelse(prob>0.5, 1, 0)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating trainning data set\n",
    "id=c(1,2,3,4,5,6)\n",
    "time_to_departure=c(20,10,15,20,80,30)\n",
    "time_since_arriving=c(5,30,15,50,50,80)\n",
    "status_on_departure=c(\"Delayed\",\"Delayed\",\"Delayed\",\"On time\",\"On time\",\"On time\")\n",
    "\n",
    "# single dataset combining multiple columns\n",
    "df=data.frame(id,time_to_departure,time_since_arriving,status_on_departure)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating trainning data set\n",
    "id=c(7,8,9)\n",
    "time_to_departure=c(30,10,20)\n",
    "time_since_arriving=c(85,5,10)\n",
    "status_on_departure=c(\"On time\",\"Delayed\",\"Delayed\")\n",
    "\n",
    "# single dataset combining multiple columns\n",
    "df=data.frame(id,time_to_departure,time_since_arriving,status_on_departure)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Machine Learning - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"https://raw.githubusercontent.com/eogasawara/mylibrary/master/myGraphics.R\")\n",
    "loadlibrary(\"randomForest\")\n",
    "loadlibrary(\"dplyr\")\n",
    "loadlibrary(\"ggraph\")\n",
    "loadlibrary(\"igraph\")\n",
    "plot_size(4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest combines the output of several decision trees and finally presents its own output. Random Forest selects all data points and variables in each of the trees. It randomly collects data points and variables in each tree it creates and then combines the output at the end. It removes the tendency that a decision tree model can introduce into the system. In addition, it significantly improves predictive power. This follows a gain in the prediction ability in relation to the decision trees by bringing the possibility of combined and random analysis among the analyzes of each tree.\n",
    "\n",
    "For our example, we created synthetic data from some observed scenarios involving the climatic conditions at the origin of a flight route, the weather conditions at the destination and the indication whether the conditions represented alert or was acceptable to perform the flight without delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating data set\n",
    "\n",
    "#weather on origin\n",
    "OrigWeather=c('goog', 'vgood', 'bad', 'bad', 'vbad', 'goog', 'bad', \n",
    "        'acceptable', 'bad', 'vbad', 'goog', 'vgood', 'acceptable', 'bad', \n",
    "        'vbad', 'goog', 'vgood', 'acceptable', 'bad', 'goog', 'goog', 'vgood',\n",
    "        'acceptable', 'acceptable', 'vbad', 'goog', 'vgood', 'acceptable', 'goog', \n",
    "        'goog')\n",
    "\n",
    "#local weather \n",
    "LocalWeather=c('goog', 'vgood', 'acceptable', 'bad', 'vbad', 'goog', 'vgood', \n",
    "         'acceptable', 'bad', 'vbad', 'goog', 'vgood', 'acceptable', 'bad', \n",
    "         'vbad', 'goog', 'vgood', 'acceptable', 'bad', 'vbad', 'goog', 'vgood',\n",
    "         'acceptable', 'bad', 'vbad', 'goog', 'vgood', 'acceptable', 'bad', \n",
    "         'vbad')\n",
    "\n",
    "#condition indication\n",
    "condition = c('acceptable', 'acceptable', 'acceptable', 'adverse', 'adverse', 'acceptable', 'acceptable', \n",
    "              'acceptable', 'adverse', 'adverse', 'acceptable', 'acceptable', 'acceptable', 'adverse', \n",
    "              'adverse', 'acceptable', 'acceptable', 'acceptable', 'adverse', 'adverse', 'acceptable', 'acceptable',\n",
    "              'acceptable', 'adverse', 'adverse', 'acceptable', 'acceptable', 'acceptable', 'adverse', \n",
    "              'adverse')\n",
    "\n",
    "\n",
    "# single dataset combining multiple columns\n",
    "df=data.frame(OrigWeather,LocalWeather, condition)\n",
    "head(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we separate the dataset between training data and data that will be tested in the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into Train and Validation sets\n",
    "# Training Set : Validation Set = 70 : 30 (random)\n",
    "set.seed(30)\n",
    "train <- sample(nrow(df), 0.7*nrow(df), replace = FALSE)\n",
    "TrainSet <- df[train,]\n",
    "ValidSet <- df[-train,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create and traine the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Random Forest model with default parameters\n",
    "model <- randomForest(TrainSet$condition ~ ., data = TrainSet, importance = TRUE)\n",
    "\n",
    "\n",
    "# Predicting on train set\n",
    "predTrain <- predict(model, TrainSet, type = \"class\")\n",
    "# Checking classification accuracy\n",
    "table(predTrain, TrainSet$condition)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting on Validation set\n",
    "predValid <- predict(model, ValidSet, type = \"class\")\n",
    "# Checking classification accuracy\n",
    "table(predValid,ValidSet$condition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the tree with max number of levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_func <- function(final_model, \n",
    "                      tree_num) {\n",
    "  \n",
    "  # get tree by index\n",
    "  tree <- randomForest::getTree(final_model, \n",
    "                                k = tree_num, \n",
    "                                labelVar = TRUE) %>%\n",
    "    tibble::rownames_to_column() %>%\n",
    "    # make leaf split points to NA, so the 0s won't get plotted\n",
    "    mutate(`split point` = ifelse(is.na(prediction), `split point`, NA))\n",
    "  \n",
    "  # prepare data frame for graph\n",
    "  graph_frame <- data.frame(from = rep(tree$rowname, 2),\n",
    "                            to = c(tree$`left daughter`, tree$`right daughter`))\n",
    "  \n",
    "  # convert to graph and delete the last node that we don't want to plot\n",
    "  graph <- graph_from_data_frame(graph_frame) %>%\n",
    "    delete_vertices(\"0\")\n",
    "  \n",
    "  # set node labels\n",
    "  V(graph)$node_label <- gsub(\"_\", \" \", as.character(tree$`split var`))\n",
    "  V(graph)$leaf_label <- as.character(tree$prediction)\n",
    "  V(graph)$split <- as.character(round(tree$`split point`, digits = 2))\n",
    "  \n",
    "  # plot\n",
    "  plot <- ggraph(graph, 'dendrogram') + \n",
    "    theme_bw() +\n",
    "    geom_edge_link() +\n",
    "    geom_node_point() +\n",
    "    geom_node_text(aes(label = node_label), na.rm = TRUE, repel = TRUE) +\n",
    "    geom_node_label(aes(label = split), vjust = 2.5, na.rm = TRUE, fill = \"white\") +\n",
    "    geom_node_label(aes(label = leaf_label, fill = leaf_label), na.rm = TRUE, \n",
    "                    repel = TRUE, colour = \"white\", fontface = \"bold\", show.legend = FALSE) +\n",
    "    theme(panel.grid.minor = element_blank(),\n",
    "          panel.grid.major = element_blank(),\n",
    "          panel.background = element_blank(),\n",
    "          plot.background = element_rect(fill = \"white\"),\n",
    "          panel.border = element_blank(),\n",
    "          axis.line = element_blank(),\n",
    "          axis.text.x = element_blank(),\n",
    "          axis.text.y = element_blank(),\n",
    "          axis.ticks = element_blank(),\n",
    "          axis.title.x = element_blank(),\n",
    "          axis.title.y = element_blank(),\n",
    "          plot.title = element_text(size = 18))\n",
    "  \n",
    "  print(plot)\n",
    "}\n",
    "\n",
    "\n",
    "# plot random tree with max number of nodes\n",
    "treenum = max(model$forest$ndbigtree)\n",
    "tree_func(model, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Network Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"https://raw.githubusercontent.com/eogasawara/mylibrary/master/myGraphics.R\")\n",
    "library(\"geosphere\")\n",
    "library(\"maps\")\n",
    "library(\"mapproj\")\n",
    "library(\"ggplot2\")\n",
    "#library(\"ggnet\")\n",
    "library(\"igraph\")\n",
    "library(\"network\")\n",
    "library(\"base\")\n",
    "library(\"sna\")\n",
    "library(\"dplyr\")\n",
    "library(\"sp\")\n",
    "library(\"ggraph\")\n",
    "library(\"GGally\")\n",
    "\n",
    "\n",
    "plot_size(4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network representation encompasses the study of flight systems according to a graph theory. Using graph theory terminology, flight networks, for example, can be modeled as a directed graph G(V,E), where V is the set of |V| nodes and E is the set of |E| edges. A node i \\in V represents a airport with a connection point. The arcs (i, j) \\in E, i \\in V and j \\in V represent a route between two airports.\n",
    "\n",
    "One of the basic measures of a vertex is its degree, defined by the number of connections with other vertices in the network, represented by the routes. In our example, the greater the degree of a vertex, the greater the number of routes that pass through that airport, which may have direct implication in its importance for the local loop or the whole system. One of the ways to identify central airports may be by their degree and, from this, take measures to mitigate the effects of a delay or problems at this point, which could impact the whole system.\n",
    "\n",
    "A well-known measure is the betweenness centrality of a node v. It is a measure aimed at summarizing the extent to which a vertex is located ‘between’ other pairs of vertices. It is a measure that quantifies the number of times an edge, in our case a route, acts as bridge along the shortest path between two other vertices.\n",
    "\n",
    "In our example, we selected data from the main airports in Brazil, including geo-positioning. From this information, we generate synthetic data simulating routes between nineteen of its main airports. A network was generated from this information, showing how it is placed on the map and its links. We calculate the degree of each airport, given the number of routes it intercepts, and the degree of centrality betweenness, showing which route is most used so that we can arrive at all points of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read list of airports in Brazil and ints GIS\n",
    "airports <- read.csv(\"brazil_airports.csv\", header = TRUE)\n",
    "airports <- airports[,2:10]\n",
    "rownames(airports) <- airports$icao\n",
    "head(airports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#read example file with flights with origin and destination airports\n",
    "routes <- read.csv(\"routes.csv\", header = TRUE)\n",
    "head(routes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to network\n",
    "routes <- network(routes, directed = TRUE)\n",
    "plot(routes)\n",
    "\n",
    "\n",
    "# add geographic coordinates\n",
    "routes %v% \"lat\" <- airports[ network.vertex.names(routes), \"lat\" ]\n",
    "routes %v% \"lon\" <- airports[ network.vertex.names(routes), \"long\" ]\n",
    "\n",
    "\n",
    "# compute degree centrality\n",
    "routes %v% \"degree\" <- degree(routes, gmode = \"digraph\")\n",
    "\n",
    "# add random groups\n",
    "routes %v% \"mygroup\" <- sample(letters[1:4], network.size(routes), replace = TRUE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load world map data\n",
    "world <- ggplot(map_data(\"world\"), aes(x = long, y = lat)) +\n",
    "  geom_polygon(aes(group = group), color = \"grey65\",\n",
    "               fill = \"#f9f9f9\", size = 0.2)\n",
    "\n",
    "# overlay network data to map\n",
    "x <- ggnetworkmap(\n",
    "  world, routes, size = 4, great.circles = TRUE,\n",
    "  node.group = mygroup, segment.color = \"steelblue\",\n",
    "  ring.group = degree, weight = degree\n",
    ")\n",
    "\n",
    "plot(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the figure above the difference of level of the number of routes that pass in each airport or until finding points of greater centrality, that would affect the whole system. In the analysis of the synthetic data, we verified that the airport with the highest degree is Brasília (SBBR) and the route most used to reach the other points of the network is the route between São Paulo and Brasília (SBGR - SBBR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Betweenness - Intermediation (Edge):\n",
    "# It is a measure that quantifies the number of times an ARESTA acts as\n",
    "#bridge along the shortest path between two other vertices.\n",
    "graph <- network_to_igraph(routes)\n",
    "b = edge.betweenness(graph)\n",
    "\n",
    "#show de most busy route betweenn two airports\n",
    "E(graph)[max(b)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show de airport with max degree\n",
    "network.vertex.names(routes)[which(routes %v% \"degree\" == max(degree(routes, gmode = \"digraph\")))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Pattern Mining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"https://raw.githubusercontent.com/eogasawara/mylibrary/master/myGraphics.R\")\n",
    "loadlibrary(\"arules\")\n",
    "loadlibrary(\"arulesViz\")\n",
    "plot_size(4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The association rule is a machine learning method with the purpose of identifying rules of association between variables using some measures of interest.\n",
    "\n",
    "One of the most used algorithms is the apriori algorithm, proposed by Agrawal et al (1993). This method assumes categorical data (does not work with numeric data), and was used initially in the analysis of the shopping basket of the supermarkets to determine how the items purchased by the customers were related.\n",
    "\n",
    "In our example, we randomly assigned twenty-three aircraft models of different manufacturers. From this list, we generated a synthetic database representing three thousand flights made with these aircrafts, also indicating the age of the aircraft (how long in service) and the indication of delay of each flight. Our objective is to verify if there is association rule between the models of the aircraft and time in which it is in service, with the fact of generating delays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read source data\n",
    "aircraft <- read.csv(\"aircraft.csv\")\n",
    "summary(aircraft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random delays and its aircrafts information\n",
    "set.seed(3000)\n",
    "flights <- data.frame(\n",
    "  aircraft = sample(aircraft[2:23, ]$aircraft, 3000, replace = TRUE),\n",
    "  model = sample(aircraft[2:23, ]$model, 3000, replace = TRUE),\n",
    "  age = sample(c(\"<5\",\">5\"), 3000, replace = TRUE),\n",
    "  delayed = sample(c(\"yes\",\"no\"), 3000, replace = TRUE)\n",
    ")\n",
    "\n",
    "#convert to factor\n",
    "flights$aircraft <-  as.factor(flights$aircraft )\n",
    "flights$model <-  as.factor(flights$model )\n",
    "flights$age <-  as.factor(flights$age )\n",
    "flights$delayed <-  as.factor(flights$delayed )\n",
    "head(flights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find association rules with default settings\n",
    "rules <- apriori(flights, parameter= list(supp=0.4, conf=0.4))\n",
    "inspect(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rules with rhs containing \"Delayed\" only\n",
    "rules <- apriori(flights,  parameter = list(minlen=2, supp=0.005, conf=0.7),\n",
    "                 appearance = list(rhs=c(\"delayed=no\", \"delayed=yes\"),default=\"lhs\"),\n",
    "                 control = list(verbose=F))\n",
    "\n",
    "rules.sorted <- sort(rules, by=\"lift\")\n",
    "inspect(rules.sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find redundant rules\n",
    "subset.matrix <- is.subset(rules.sorted, rules.sorted)\n",
    "subset.matrix[lower.tri(subset.matrix, diag=T)] <- NA\n",
    "redundant <- colSums(subset.matrix, na.rm=T) >= 1\n",
    "which(redundant)\n",
    "\n",
    "# remove redundant rules\n",
    "rules.pruned <- rules.sorted[!redundant]\n",
    "inspect(rules.pruned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(rules)\n",
    "plot(rules, method=\"graph\", control=list(type=\"items\"))\n",
    "plot(rules, method=\"paracoord\", control=list(reorder=TRUE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining minimum levels of support and trust for the analysis, we found 5 association rules for cases where there was a delay, showing which aircraft models and time in service were related to these delays. The second figure above shows associated delay information, in the centre, the rules, represented by the spheres. the size of each sphere is proportional to the support and confidence of the information in relation to the delay."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source(\"https://raw.githubusercontent.com/eogasawara/mylibrary/master/myGraphics.R\")\n",
    "loadlibrary(\"tidyverse\")\n",
    "loadlibrary(\"ggpubr\")\n",
    "loadlibrary(\"tidyverse\")\n",
    "loadlibrary(\"ggplot2\")\n",
    "plot_size(4, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression analysis studies the relationship between a variable called the variable dependent variables and other variables called independent variables. The relation between them is represented by a mathematical model if association between the variables and can provide prediction of future behavior. One of the regression models, the simple linear regression model, defines a linear relationship between the dependent variable and a variable independent. If multiple independent variables are involved, it would be called a multiple linear regression model. This prediction can be achieved through a study involving the equation of (y, dependent or response) and independent (x, also known as prognosis) variables.\n",
    "\n",
    "We set up a dataset with ten flight arrivals, with information representing time between the touch of tires on the ground and another field representing the time delay recorded on the flight. The purpose here is to know if there is a relationship between the two pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wheelson =    c(20,-10,  5,  2,  8, 30, -10, 5, 4, 6)\n",
    "\n",
    "timedelayed = c(30,  5, 15, 10, 15, 20,  0, 25, 10, 18)\n",
    "\n",
    "# single dataset combining multiple columns\n",
    "df=data.frame(wheelson, timedelayed)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_set(theme_pubr())\n",
    "\n",
    "model <- lm(wheelson ~ timedelayed, data = df)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ggplot(df, aes(timedelayed, wheelson)) +\n",
    "  geom_point() +\n",
    "  stat_smooth(method = lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph we can visualize that there is a relationship between the variables, and that possibly a forecasting with a good level of accuracy, the total delay times of a flight through the delay between the tire touch time on the ground and the arrival at the gate. And, if true, as in the hypothetical case presented, it could indicate a problem in the operation of the airport, where the ground time since arrival includes the total time of delay, in cases of delay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
